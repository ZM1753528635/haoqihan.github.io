<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux基础命令进阶]]></title>
    <url>%2F2019%2F01%2F16%2Flinux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[linux基础命令]]></title>
    <url>%2F2019%2F01%2F16%2Flinux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[IP相关 ip addr ifconfig 关闭防火墙 systemctl stop / start / restart firewalld 关闭/开启/重启 防火墙 systemctl disable firewalld 永久关闭防火墙 修改网络配置 systemctl restart network 重启服务 linux的目录结构 根目录 linux的文件系统 Ext3 : 是一款日志文件系统，能够在系统异常宕机时避免文件系统资料丢失，并能 自动修复数据的不一致与错误。 Etx4:Etx3的进阶版本,作为 RHEL 6 系统中的默认文件管理系统，它支持的存储容 量高达 1EB(1EB=1,073,741,824GB)，且能够有无限多的子目录。另外，Ext4 文件系统能够批量分配 block 块，从而极大地提高了读写效率。 XFS 是一种高性能的日志文件系统，而且是 RHEL 7 中默认的文件管理系统，它的优势在发生意外宕机后尤其明显，即可以快速地恢复可能被破坏的文件，而且强大的 日志功能只用花费极低的计算和存储性能。并且它最大可支持的存储容量为 18EB， 这几乎满足了所有需求。 查看linux的文件系统 cat /etc/fstab 创建文件夹 mkdir -p 文件名 查看目录 ls /目录名 stat 文件名:查看详细信息 创建文本 touch 文件名 改变当前的位置 cd /目录名 cd ~ :家目录 cd - :返回上一次的地址 打印当前目录 pwd 查看文本 cat -n 文件 :带行号 cat &gt;&gt; 文件 &lt;&lt; EOF …. EOF more 查看文件百分比 head 查看前10行 tail 查看后10行 tail -f 动态监听文件 linux快捷键 快捷键 说明 tab键 自动补全代码 ctrl + l 清理终端显示 clear / cls 清理终端显示 ctrl + c 终止操作 echo命令 echo $PATH :打印环境变量 特殊符号 “&gt;&gt;” 追加重定向 “&gt;” 清空重定向 “*” 通配符 复制 cp -r 递归,复制目录以及目录的子孙后代 cp -p 复制文件,且保持文件属性不变 cp -a :相当于-pdr 移动 mv 删除 rm -f 不需要提示,强制删除 rm -rf :全删 xargs命令 xargs命令是给其他命令传递参数的一个过滤器，擅长将标准输入数据转换成命令行参数，xargs能够处理管道或者stdin并将其转换成特定命令的参数。 xargs默认命令是echo，空格是默认定界符 查找命令 find 在哪里(目录) -type f -name ‘*.txt’ 找到所有txt文件 find / -name *.txt 管道命令 Linux提供的管道符“|”讲两条命令隔开，管道符左边命令的输出会作为管道符右边命令的输入。 命令格式:命令A|命令B grep(过滤) 文本搜素 grep ‘xxx’ 文件 -i:忽略大小写 -n:输出行号 -v:反向选择 sed(流编辑器) 用法 文本替换 sed -i ‘s/old/new/g’ 文件:把这文件里的全部old替换为new 删除空白行 sed -i ‘/^$/d’ 文件名 :把这个文件里全部的空行删除 删除5-10行内容 sed -i ‘5,10d’ 文件名 akw 语法: awk [option] ‘script’ var = value filename awk [options] -f scriptfile var=value filename 常用选项 -F fs fs指定输入分隔符，fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk -f scripfile 从脚本文件中读取awk命令 -m[fr] val 对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 实例 # NR &gt; 行号 awk ‘NR==20,NR==30’ /tmp/oldboy.txt which命令 which命令用于查找并显示给定命令的绝对路径，环境变量PATH中保存了查找命令时需要遍历的目录。 which指令会在环境变量$PATH设置的目录里查找符合条件的文件。 也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 which pwd which python 别名alias 设置别名 alias rm = ‘echo “别用rm”‘ 取消设置:unalias rm seq(类似于for循环) seq -f ‘%03g’ 起始值 终止值 scp远程复制文件 语法: scp 【可选参数】 本地源文件 远程文件标记 参数 -r :递归复制整个目录 -v:详细方式输出 -q:不显示传输进度条 -C：允许压缩 事例 传输本地文件到远程地址 scp 本地文件 远程用户名@远程ip:远程文件夹/ 复制远程文件到本地 scp root@192.168.1.155:/home/oldboy.txt /tmp/oldboy.txt du命令 显示目录和文件大小 实例: du -sh 文件名 top top命令用于动态地件事进程活动与系统负载等信息 第一行 (uptime) 系统时间 主机运行时间 用户连接数(who) 系统1，5，15分钟的平均负载 第二行:进程信息 进程总数 正在运行的进程数 睡眠的进程数 停止的进程数 僵尸进程数 第三行:cpu信息 1.5 us：用户空间所占CPU百分比 0.9 sy：内核空间占用CPU百分比 0.0 ni：用户进程空间内改变过优先级的进程占用CPU百分比 97.5 id：空闲CPU百分比 0.2 wa：等待输入输出的CPU时间百分比 0.0 hi：硬件CPU中断占用百分比 0.0 si：软中断占用百分比 0.0 st：虚拟机占用百分比 第四行：内存信息（与第五行的信息类似与free命令） 8053444 total：物理内存总量 7779224 used：已使用的内存总量 274220 free：空闲的内存总量（free+used=total） 359212 buffers：用作内核缓存的内存量 第五行：swap信息 8265724 total：交换分区总量 33840 used：已使用的交换分区总量 8231884 free：空闲交换区总量 4358088 cached Mem：缓冲的交换区总量，内存中的内容被换出到交换区，然后又被换入到内存，但是使用过的交换区没有被覆盖，交换区的这些内容已存在于内存中的交换区的大小，相应的内存再次被换出时可不必再对交换区写入。 chattr 给文件加锁,只能写数据,无法删除 chattr +a 文件:给文件加锁 chattr -a 文件:给文件取消锁 lsattr 查看文件的隐藏属性 lsattr 文件名 linux时间同步 date +”%Y-%m-%d %T” 显示当前时间 同步系统时间和硬件时间，可以用hwclock命令 //以系统时间为基准，修改硬件时间 [root@oldboy_python ~ 10:29:07]#hwclock-w //以硬件时间为基准，修改系统时间 [root@oldboy_python ~ 10:29:21]#hwclock-s Ntp时间服务器 ntpdate -u ntp. aliyun. com:更新时间 wget命令 wget 参数 下载地址 开关机命令 reboot 命令用于重启机器 poweroff 用于关闭系统]]></content>
      <categories>
        <category>linux的使用</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tornado框架]]></title>
    <url>%2F2019%2F01%2F15%2FTornado%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django-celery的基本使用]]></title>
    <url>%2F2019%2F01%2F15%2FDjango-celery%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>分布式任务</category>
      </categories>
      <tags>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[celery的基本使用]]></title>
    <url>%2F2019%2F01%2F15%2Fcelery%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[celery是什么?​ celery是一个基于python实现的模块,模块可以帮助我们实现任务管理 什么情况下使用celery?一个请求的处理时间特别长,可以使用celery(例如:发送邮件,短信等) 安装celery1pip install celery 快速入门目录结构123456- celery_app -- __init__.py -- celeryconfig.py -- task1.py -- task2.py--demo.py init .py 1234from celery import Celeryapp = Celery('demo') # demo名称,可以是任意修改app.config_from_object('celery_app.celeryconfig') # 通过celery实例加载配置模块 celeryconfig.py配置文件 123456789101112131415161718192021222324252627282930from datetime import timedeltafrom celery.schedules import crontabBROKER_URL = 'redis://localhost:6379/1' # 设置broker存储的位置CELERY_RESULT_BACKEND = 'redis://localhost:6379/0' #设置backend的位置CELERY_TIMEZONE = 'Asia/Shanghai' # 设置时区# UTC# 导入指定的任务模块CELERY_IMPORTS = ( 'celery_app.task1', 'celery_app.task2')# 定时任务CELERYBEAT_SCHEDULE = &#123; 'task1':&#123; 'task':'celery_app.task1.add', 'schedule': timedelta(seconds=10), # 每10秒执行一次 'args':(2, 8) &#125;, 'task2':&#123; 'task':'celery_app.task2.multiply', 'schedule': crontab(hour=17,minute=27), # 每天的17点27分执行一次 'args':(4,5) &#125;&#125; task1.py 12345from celery_app import app@app.task # 设置一个处理任务的函数def add(x,y): return x + y task2.py 123456from celery_app import app@app.taskdef multiply(x , y): # 设置第二个 return x * y demo.py 12345678910111213141516171819# 发起任务from celery_app import task1from celery_app import task2# 第一种 参数:args是存放数据 eta=datetime(2018, 4, 11, 2, 32, 0):可以设置定时任务task1.add.apply_async() task1.add.delay(2,4)xx = task2.multiply.delay(4,5) # 第二种 直接传入参数# 获取结果from celery.result import AsyncResultfrom celery_app import appres = AsyncResult(id=xx.id,app=app)if res.ready(): # 判断结果是否返回 return res.get() # 返回的话把结果返回]]></content>
      <categories>
        <category>分布式任务</category>
      </categories>
      <tags>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zeroMQ的基本使用]]></title>
    <url>%2F2019%2F01%2F15%2FzeroMQ%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>zeroMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka的基本使用]]></title>
    <url>%2F2019%2F01%2F15%2Fkafka%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy的使用]]></title>
    <url>%2F2019%2F01%2F15%2FSQLAlchemy%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是sqlalchemy?SQLAlchemy是Python编程语言下的一款ORM框架，该框架建立在数据库API之上，使用关系对象映射进行数据库操作，简言之便是：将对象转换成SQL，然后使用数据API执行SQL并获取执行结果。 连接数据库首先需要导入 sqlalchemy 库，然后建立数据库连接，这里使用 mysql。通过create_engine方法进行 12from sqlalchemy import create_engineengine = create_engine("mysql://root:@localhost:3306/webpy?charset=utf8",encoding="utf-8", echo=True) 基本使用1234567891011121314151617181920212223242526272829303132333435363738394041from sqlalchemy import Column, String, create_enginefrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.ext.declarative import declarative_base# 创建对象的基类:Base = declarative_base()# 定义User对象:class User(Base): # 表的名字: __tablename__ = 'user' # 表的结构: id = Column(String(20), primary_key=True) name = Column(String(20))# 初始化数据库连接:engine = create_engine('mysql+mysqlconnector://root:password@localhost:3306/test')# 创建DBSession类型:DBSession = sessionmaker(bind=engine)# 创建session对象:session = DBSession()# 创建新User对象:new_user = User(id='5', name='Bob')# 添加到session:session.add(new_user)# 提交即保存到数据库:session.commit()# 关闭session:session.close()# 创建Session:session = DBSession()# 创建Query查询，filter是where条件，最后调用one()返回唯一行，如果调用all()则返回所有行:user = session.query(User).filter(User.id=='5').one()# 打印类型和对象的name属性:print('type:', type(user))print('name:', user.name)# 关闭Session:session.close() 引用: https://github.com/michaelliao/learn-python3/blob/master/samples/db/do_sqlalchemy.py]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>sqlAlchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Memcache的使用]]></title>
    <url>%2F2019%2F01%2F15%2FMemcache%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是Memcached?Memcached 是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提高动态、数据库驱动网站的速度。Memcached基于一个存储键/值对的hashmap。其守护进程（daemon ）是用C写的，但是客户端可以用任何语言来编写，并通过memcached协议与守护进程通信。 安装和参数Memcached的安装12345678wget http://memcached.org/latesttar -zxvf memcached-1.x.x.tar.gzcd memcached-1.x.x./configure &amp;&amp; make &amp;&amp; make test &amp;&amp; sudo make install PS：依赖libevent yum install libevent-devel apt-get install libevent-dev 启动Memcached12345678910memcached -d -m 10 -u root -l 10.211.55.4 -p 12000 -c 256 -P /tmp/memcached.pid 参数说明: -d 是启动一个守护进程 -m 是分配给Memcache使用的内存数量，单位是MB -u 是运行Memcache的用户 -l 是监听的服务器IP地址 -p 是设置Memcache监听的端口,最好是1024以上的端口 -c 选项是最大运行的并发连接数，默认是1024，按照你服务器的负载量来设定 -P 是设置保存Memcache的pid文件 python操作Memcached12# 下载pip install python-memcached 第一次操作12345678import memcachemc = memcache.Client(['127.0.0.1:11211'], debug=True)mc.set("foo", "bar")ret = mc.get('foo')print(ret)# debug = True 表示运行出现错误时，现实错误信息，上线后移除该参数。 天生支持集群python-memcached模块原生支持集群操作，其原理是在内存维护一个主机列表，且集群中主机的权重值和主机在列表中重复出现的次数成正比 1234567891011 主机 权重 1.1.1.1 1 1.1.1.2 2 1.1.1.3 1 那么在内存中主机列表为： host_list = ["1.1.1.1", "1.1.1.2", "1.1.1.2", "1.1.1.3", ]# 代码mc = memcache.Client([('1.1.1.1:12000', 1), ('1.1.1.2:12000', 2), ('1.1.1.3:12000', 1)], debug=True)mc.set('k1', 'v1') add添加一条键值对,如果已经存在key,重复执行add操作异常 12345import memcache mc = memcache.Client(['10.211.55.4:11211'], debug=True)mc.add('k1', 'v1')# mc.add('k1', 'v2') # 报错，对已经存在的key重复添加，失败！！！ replacereplace修改某个key的值,如果key不存在则异常 12345import memcache mc = memcache.Client(['10.211.55.4:11211'], debug=True)# 如果memcache中存在kkkk，则替换成功，否则异常mc.replace('kkkk','999') set和set_multiset 设置值一个键值对,如果key不存在,则创建,如果key存在,则修改 set_multi 设置多个键值对,如果key不存在,则创建,如果key存在,则修改 1234import memcachemc = memcache.Client(['10.211.55.4:11211'], debug=True)mc.set('key0', 'wupeiqi')mc.set_multi(&#123;'key1': 'val1', 'key2': 'val2'&#125;) delete 和 delete_multidelete 在Memcached中删除指定一个键值对 delete_multi 在Memcached中删除指定的多个键值对 123456import memcache mc = memcache.Client(['10.211.55.4:11211'], debug=True) mc.delete('key0')mc.delete_multi(['key1', 'key2']) get 和 get_multiget 获取一个键值对 get_multi 获取多个键值对 1234import memcachemc = memcache.Client(['10.211.55.4:11211'], debug=True)val = mc.get('key0')item_dict = mc.get_multi(["key1", "key2", "key3"]) append 和 prependappend 修改指定key的值,在该值后面追加内容 prepend 修改指定key的值,在该值前面插入内容 12345678910import memcache mc = memcache.Client(['10.211.55.4:11211'], debug=True)# k1 = "v1" mc.append('k1', 'after')# k1 = "v1after" mc.prepend('k1', 'before')# k1 = "beforev1after" decr和incrincr 自增,将Memcached中的某一个值增加N(N默认是1) decr 自减,将Memcached中的某一个值减少N(N默认是1) 12345678910111213141516import memcache mc = memcache.Client(['10.211.55.4:11211'], debug=True)mc.set('k1', '777') mc.incr('k1')# k1 = 778 mc.incr('k1', 10)# k1 = 788 mc.decr('k1')# k1 = 787 mc.decr('k1', 10)# k1 = 777 gets 和 cas如商城商品剩余个数，假设改值保存在memcache中，product_count = 900A用户刷新页面从memcache中读取到product_count = 900B用户刷新页面从memcache中读取到product_count = 900 如果A、B用户均购买商品 A用户修改商品剩余个数 product_count＝899B用户修改商品剩余个数 product_count＝899 如此一来缓存内的数据便不在正确，两个用户购买商品后，商品剩余还是 899如果使用python的set和get来操作以上过程，那么程序就会如上述所示情况！ 如果想要避免此情况的发生，只要使用 gets 和 cas 即可，如： 1234567import memcachemc = memcache.Client(['10.211.55.4:12000'], debug=True, cache_cas=True) v = mc.gets('product_count')# ...# 如果有人在gets之后和cas之前修改了product_count，那么，下面的设置将会执行失败，剖出异常，从而避免非正常数据的产生mc.cas('product_count', "899") ​ 本质上每次执行gets时，会从memcache中获取一个自增的数字，通过cas去修改gets的值时，会携带之前获取的自增值和memcache中的自增值进行比较，如果相等，则可以提交，如果不想等，那表示在gets和cas执行之间，又有其他人执行了gets（获取了缓冲的指定值）， 如此一来有可能出现非正常数据，则不允许修改]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mangoDB基础]]></title>
    <url>%2F2019%2F01%2F14%2FmangoDB%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[数据库中的用法MangoDB的对应关系引用了不存在的对象来创建改对象 1234database ------&gt; Databasetable -------&gt; Collection字段 --------&gt; Fieldrow ---------&gt; Document mangoDB增删改查123456789101112131415161718增: 官方不推荐写法: db.users.insert([&#123;&#125;]) db.users.insert(&#123;&#125;) 官方支持写法 db.users.insertMany([&#123;'name':'ccc','age':88&#125;,&#123;'name':'ddd','age':88&#125;]) db.users.insertOne(&#123;name:"xxx",age:"73"&#125;)查: db.users.find(&#123;age:73,name:"xxx"&#125;) db.users.findOne(&#123;age:73&#125;)改：MongoDB修改器 $set $unset:删除字段的 db.users.updateOne(&#123;age:73&#125;,&#123;$set:&#123;age:74&#125;&#125;) db.users.updateMany(&#123;age:74&#125;,&#123;$set:&#123;age:73&#125;&#125;) 删: db.users.deleteOne(&#123;age:"84"&#125;) db.users.deleteMany(&#123;age:"84"&#125;) MongoDB的数据类型12345678910Object ID ：Documents 自生成的 _idString： 字符串，必须是utf-8Boolean：布尔值，true 或者false (这里有坑哦~在Python中 True False 首字母大写)Integer：整数 (Int32 Int64 你们就知道有个Int就行了,一般我们用Int32)Double：浮点数 (没有float类型,所有小数都是Double)Arrays：数组或者列表，多个值存储到一个键 (list哦,大Python中的List哦)Object：如果你学过Python的话,那么这个概念特别好理解,就是Python中的字典,这个数据类型就是字典Null：空数据类型 , 一个特殊的概念,None NullTimestamp：时间戳Date：存储当前日期或时间unix时间格式 (我们一般不用这个Date类型,时间戳可以秒杀一切时间类型) $关键字12345678910111213141516171819202122232425修改器 $set : 强制覆盖 $unset : 删除字段 $inc ：引用自增 $inc:&#123;age:-1&#125; $push append(7) db.sss.updateOne(&#123;name:"xxx"&#125;,&#123;$push:&#123;hobby_1:7&#125;&#125;) $pull remove(1) db.sss.updateOne(&#123;name:"xxx"&#125;,&#123;$pull:&#123;hobby_1:1&#125;&#125;) $pop pop() db.sss.updateOne(&#123;name:"xxx"&#125;,&#123;$pop:&#123;hobby_1:1/-1&#125;&#125;) 1删除最后一个,-1代表删除第一个 查询关键： $or $or:[&#123;age:1&#125;,&#123;name:2&#125;] $all &#123;u_list:&#123;$all:[321,123]&#125;&#125; $in &#123;age:&#123;$in:[10,15]&#125;&#125; 数学比较符： $lt &#123;age:&#123;$gt:10&#125;&#125; $lte $gt $gte $eq : $ne &#123;age:&#123;$ne:15&#125;&#125; 4. $ (&#123;name:"xxx","hobby.name":"个人计算机"&#125;,&#123;$set:&#123;"hobby.$.name":"PC"&#125;&#125;) skip limit sort12345sort: sort(&#123; age:1 / -1&#125;) -1:倒序 1:正序 skip: skip(2) 跳过两条limit： limit(2) 选取两条优先级: 1.sort 2.skip 3.limit python中使用MangoDB连接MongoDBClient12import pymongoclient = pymongo.MongoClient(host='localhost', port=27017) 获取数据库12db = Client.test_databasedb = Client['test_database'] 获取CollectionCollection是存储在MongoDB中的一组文件，同获取database一样，你可以用点取属性的方式或者字典的方法获取： 12collection = db.test_collectioncollection = db['test_collection'] 其他操作和上面一样]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MangoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis进阶]]></title>
    <url>%2F2019%2F01%2F14%2Fredis%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[什么是redisredis是一个基于内存的高性能key-value数据库 2.redis的特点Redis本质是一个key-value类型的数据库,就像memcached,整个数据库统统加载在内存中进行操作,定期通过异步把数据库数据push到硬盘上进行保存,因为是纯内存操作所以redis的性能非常出色,每秒可以处理10万次读写操作,是已知性能最快的key-value DB Redis的出色之处不仅仅只有他的性能,Redis最大的魅力是支持多种数据结构,此外单个value的最大限制是1GB,不像memcached只能保存1MB的数据,因此redis可以用来实现很多有用的功能,比如:用他的List来做FIFO双向链表实现一个轻量级的高性能消息队列服务,用他的set可以做高性能的tag系统等等,另外Redis也可以对存入的key-value设置expire(过期)时间,因此也可以当做一个加强版的memcached来用. Redis主要缺点是数据库容量受物理内存限制,不能做海量数据的高性能读写,因此它只适合在较小数据量的高性能操作和运算上 PS—–&gt;memcached:是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提高动态、数据库驱动网站的速度。Memcached基于一个存储键/值对的hashmap。其守护进程（daemon ）是用C写的，但是客户端可以用任何语言来编写，并通过memcached协议与守护进程通信。 3.Redis支持的数据类型Redis通过key-value的单值不同类型区分 strings Lists sets sorted set hashes 4.为什么redis需要把所有数据放到内存中redis为了达到最快的读写速度将数据都读到内存中,并通过异步的方式将数据写入磁盘,所以redis具有快速和数据持久化的特性,如果不将数据放在内存中,磁盘I/O速度会影响redis的性能, 如果设置最大使用的内存,则数据已有记录达到内存限值后不能继续插入新值 5.Redis是单进程单线程的redis利用队列技术将并发访问变成为串行访问,消除了传统数据库串行控制开销 6.虚拟内存当你的key很小而value很大时,使用vm的效果会比较好,因为这样节约内存比较大 当你的key不小时,可以考虑一些非常方法将很大的key变成value,比如你可以将key-value变成一个value vm-max-threads这个参数,可以设置访问swap文件的线程数,设置最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证. 7.分布式redis支持主从的模式,原则:master(控制)会将数据同步到slave,而slave不会讲数据同步到master,slave启动时会连接master来同步数据 这是一个典型的分布式读写分离模型,我们可以利用master来插入数据,slave提供检索服务,这样可以有效减少单个机器的并发访问数量 8.读写分离模型通过增加slave DB的数量,读的性能可以线性增长,为了避免master DB的单点故障,集群一般都会采用两台master DB做双机热备所以整个集群的读和写的可用性都非常高 读写分离的缺陷:不管master还是slave,每个节点都必须保存完整的数据,如果在数据量很大的时候,集群的扩展能力还是受限于每个节点 的存储能力,而且对于write-intensive类型的应用,读写分离的架构并不合适 9.数据分片模型为了解决读写分离模型的缺陷,可以将数据分片模型应用进来 可以将每个节点都看成独立的master,然后通过业务实现数据分片 结合上面两种模型,可以将每个master设计由一个master和多个slave组成的模型 10.Redis的回收策略 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 11.使用redis有哪些好处 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) 支持丰富数据类型，支持string，list，set，sorted set，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 12.redis相比memcached有哪些优势？ memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 redis的速度比memcached快很多 redis可以持久化其数据 13.redis常见性能问题和解决方案： Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内 尽量避免在压力很大的主库上增加从库 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3… 这样的结构方便解决单点故障问题，实现Slave对Master的替换。如果Master挂了，可以立刻启用Slave1做Master，其他不变 14.MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据相关知识：redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。详情见 redis的回收策略 15.redis常见的性能问题有哪些?如何解决 Master写内存快照，save命令调度rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以Master最好不要写内存快照。 Master AOF持久化，如果不重写AOF文件，这个持久化方式对性能的影响是最小的，但是AOF文件会不断增大，AOF文件过大会影响Master重启的恢复速度。Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化,如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 Redis主从复制的性能问题，为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内 16.redis适合的场景Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢? Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 、Redis支持数据的备份，即master-slave模式的数据备份。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 17、会话缓存（Session Cache）最常用的一种使用Redis的情景是会话缓存（session cache）。用Redis缓存会话比其他存储（如Memcached）的优势在于：Redis提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用Redis来缓存会话的文档。甚至广为人知的商业平台Magento也提供Redis的插件。 18、全页缓存（FPC）除基本的会话token之外，Redis还提供很简便的FPC平台。回到一致性问题，即使重启了Redis实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似PHP本地FPC。 再次以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。 此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 19、队列Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。Redis作为队列使用的操作，就类似于本地程序语言（如Python）对 list 的 push/pop 操作。 如果你快速的在Google中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用Redis创建非常好的后端工具，以满足各种队列需求。例如，Celery有一个后台就是使用Redis作为broker，你可以从这里去查看。 20，排行榜/计数器Redis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的10个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行： ZRANGE user_scores 0 10 WITHSCORES 21、发布/订阅最后（但肯定不是最不重要的）是Redis的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用Redis的发布/订阅功能来建立聊天系统！（不，这是真的，你可以去核实）。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis基础]]></title>
    <url>%2F2019%2F01%2F14%2Fredis%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[redis的数据类型 string(字符串) Hash(哈希/字典) List(数组/列表) 无序集合 有序集合 安装1pip install redi 操作模式redis-py提供两个类Redis和StrictRedis用于实现Redis的命令，StrictRedis用于实现大部分官方的命令，并使用官方的语法和命令，Redis是StrictRedis的子类，用于向后兼容旧版本的redis-py。 1234import redisr = redis.Redis(host='10.211.55.4', port=6379)r.set('foo', 'Bar')print r.get('foo') 连接池redis-py使用connection pool来管理对一个redis server的所有连接，避免每次建立、释放连接的开销。默认，每个Redis实例都会维护一个自己的连接池。可以直接建立一个连接池，然后作为参数Redis，这样就可以实现多个Redis实例共享一个连接池。 12345import redispool = redis.ConnectionPool(host='10.211.55.4', port=6379)r = redis.Redis(connection_pool=pool)r.set('foo', 'Bar')print r.get('foo') string的基本操作set(name,value,ex=None,px=None,nx=False,xx=False) 123456在Redis中设置值，默认，不存在则创建，存在则修改参数： ex，过期时间（秒） px，过期时间（毫秒） nx，如果设置为True，则只有name不存在时，当前set操作才执行 xx，如果设置为True，则只有name存在时，岗前set操作才执行 setnx(name, value)设置值，只有name不存在时，执行设置操作（添加） setex(name, value, time) :time，过期时间（数字秒 或 timedelta对象） psetex(name, time_ms, value) :time_ms，过期时间（数字毫秒 或 timedelta对象） mset(*args, **kwargs) 12345批量设置值如： mset(k1='v1', k2='v2') 或 mget(&#123;'k1': 'v1', 'k2': 'v2'&#125;) get(name):获取值 mget(keys, *args) 12345批量获取如： mget('ylr', 'wupeiqi') 或 r.mget(['ylr', 'wupeiqi']) getset(name, value) :设置新值并获取原来的值 getrange(key, start, end) : 12345# 获取子序列（根据字节获取，非字符）# 参数： # name，Redis 的 name # start，起始位置（字节） # end，结束位置（字节） setrange(name, offset, value) 1234修改字符串内容，从指定字符串索引开始向后替换（新值太长时，则向后添加）参数： # offset，字符串的索引，字节（一个汉字三个字节） # value，要设置的值 setbit(name, offset, value) 12345678910111213141516171819202122232425 对name对应值的二进制表示的位进行操作 参数： # name，redis的name # offset，位的索引（将值变换成二进制后再进行索引） # value，值只能是 1 或 0 注：如果在Redis中有一个对应： n1 = "foo"， 那么字符串foo的二进制表示为：01100110 01101111 01101111 所以，如果执行 setbit('n1', 7, 1)，则就会将第7位设置为1， 那么最终二进制则变成 01100111 01101111 01101111，即："goo" 扩展，转换二进制表示： # source = "郝起瀚" source = "foo" for i in source: num = ord(i) print bin(num).replace('b','') 特别的，如果source是汉字 "郝起瀚"怎么办？ 答：对于utf-8，每一个汉字占 3 个字节，那么 "武沛齐" 则有 9个字节 对于汉字，for循环时候会按照 字节 迭代，那么在迭代时，将每一个字节转换 十进制数，然后再将十进制数转换成二进制 11100110 10101101 10100110 11100110 10110010 10011011 11101001 10111101 10010000 getbit(name, offset) 1获取name对应的值的二进制表示中的某位的值 （0或1） bitcount(key, start=None, end=None) 12345获取name对应的值的二进制表示中 1 的个数参数： # key，Redis的name # start，位起始位置 # end，位结束位置 bitop(operation, dest, *keys) 12345678910获取多个值，并将值做位运算，将最后的结果保存至新的name对应的值 参数： operation,AND（并） 、 OR（或） 、 NOT（非） 、 XOR（异或） dest, 新的Redis的name *keys,要查找的Redis的name 如： bitop("AND", 'new_name', 'n1', 'n2', 'n3') 获取Redis中n1,n2,n3对应的值，然后讲所有的值做位运算（求并集），然后将结果保存 new_name 对应的值中 strlen(name) 1返回name对应值的字节长度（一个汉字3个字节） incr(self, name, amount=1) 1234567 自增 name对应的值，当name不存在时，则创建name＝amount，否则，则自增。 参数： # name,Redis的name # amount,自增数（必须是整数） 注：同incrby incrbyfloat(self, name, amount=1.0) 12345自增 name对应的值，当name不存在时，则创建name＝amount，否则，则自增。 参数： name,Redis的name amount,自增数（浮点型） decr(self, name, amount=1) 12345自减 name对应的值，当name不存在时，则创建name＝amount，否则，则自减。参数： name,Redis的name amount,自减数（整数） append(key, value) 12345在redis name对应的值后面追加内容 参数： key, redis的name value, 要追加的字符串 Hash操作hset(name,key,value) 123456789name对应的hash中设置一个键值对（不存在，则创建；否则，修改） 参数： name，redis的name key，name对应的hash中的key value，name对应的hash中的value 注： hsetnx(name, key, value),当name对应的hash中不存在当前key时则创建（相当于添加） hmset(name, mapping) 1234567在name对应的hash中批量设置键值对 参数： name，redis的name mapping，字典，如：&#123;'k1':'v1', 'k2': 'v2'&#125;如： r.hmset('xx', &#123;'k1':'v1', 'k2': 'v2'&#125;) hget(name,key) 1在name对应的hash中获取根据key获取value hmget(name, keys, *args) 1234567891011在name对应的hash中获取多个key的值 参数： name，reids对应的name keys，要获取key集合，如：['k1', 'k2', 'k3'] *args，要获取的key，如：k1,k2,k3 如： r.mget('xx', ['k1', 'k2']) 或 print r.hmget('xx', 'k1', 'k2') hgetall(name) 1获取name对应hash的所有键值 hlen(name) 1获取name对应的hash中键值对的个数 hkeys(name) 1获取name对应的hash中所有的key的值 hvals(name) 1获取name对应的hash中所有的value的值 hexists(name, key) 1检查name对应的hash是否存在当前传入的key hdel(name,*keys) 1将name对应的hash中指定key的键值对删除 hincrby(name, key, amount=1) 12345自增name对应的hash中的指定key的值，不存在则创建key=amount参数： name，redis中的name key， hash对应的key amount，自增数（整数） hincrbyfloat(name, key, amount=1.0) 1234567自增name对应的hash中的指定key的值，不存在则创建key=amount 参数： name，redis中的name key， hash对应的key amount，自增数（浮点数）自增name对应的hash中的指定key的值，不存在则创建key=amount hscan(name, cursor=0, match=None, count=None) 1234567891011增量式迭代获取，对于数据大的数据非常有用，hscan可以实现分片的获取数据，并非一次性将数据全部获取完，从而放置内存被撑爆 参数： name，redis的name cursor，游标（基于游标分批取获取数据） match，匹配指定key，默认None 表示所有的key count，每次分片最少获取个数，默认None表示采用Redis的默认分片个数 如： 第一次：cursor1, data1 = r.hscan('xx', cursor=0, match=None, count=None) 第二次：cursor2, data1 = r.hscan('xx', cursor=cursor1, match=None, count=None) hscan_iter(name, match=None, count=None) 123456789利用yield封装hscan创建生成器，实现分批去redis中获取数据 参数： match，匹配指定key，默认None 表示所有的key count，每次分片最少获取个数，默认None表示采用Redis的默认分片个数 如： for item in r.hscan_iter('xx'): print item List操作redis中的List在在内存中按照一个name对应一个List来存储。 lpush(name,values) 12345678在name对应的list中添加元素，每个新的元素都添加到列表的最左边 如： r.lpush('oo', 11,22,33) 保存顺序为: 33,22,11 扩展： rpush(name, values) 表示从右向左操作 lpushx(name,value) 1234在name对应的list中添加元素，只有name已经存在时，值添加到列表的最左边 更多： rpushx(name, value) 表示从右向左操作 llen(name) 1name对应的list元素的个数 linsert(name, where, refvalue, value)) 1234567在name对应的列表的某一个值前或后插入一个新值 参数： name，redis的name where，BEFORE或AFTER refvalue，标杆值，即：在它前后插入数据 value，要插入的数据 r.lset(name, index, value) 123456对name对应的list中的某一个索引位置重新赋值 参数： name，redis的name index，list的索引位置 value，要设置的值 r.lrem(name, value, num) 12345678在name对应的list中删除指定的值 参数： name，redis的name value，要删除的值 num， num=0，删除列表中所有的指定值； num=2,从前到后，删除2个； num=-2,从后向前，删除2个 lpop(name) 1234在name对应的列表的左侧获取第一个元素并在列表中移除，返回值则是第一个元素 更多： rpop(name) 表示从右向左操作 lindex(name, index) 1在name对应的列表中根据索引获取列表元素 lrange(name, start, end) 12345在name对应的列表分片获取数据 参数： name，redis的name start，索引的起始位置 end，索引结束位置 ltrim(name, start, end) 12345在name对应的列表中移除没有在start-end索引之间的值参数： name，redis的name start，索引的起始位置 end，索引结束位置 rpoplpush(src, dst) 1234从一个列表取出最右边的元素，同时将其添加至另一个列表的最左边 参数： src，要取数据的列表的name dst，要添加数据的列表的name blpop(keys, timeout) 12345将多个列表排列，按照从左到右去pop对应列表的元素 参数： keys，redis的name的集合 timeout，超时时间，当元素所有列表的元素获取完之后，阻塞等待列表内有数据的时间（秒）, 0 表示永远阻塞 brpoplpush(src, dst, timeout=0) 123456从一个列表的右侧移除一个元素并将其添加到另一个列表的左侧 参数： src，取出并要移除元素的列表对应的name dst，要插入元素的列表对应的name timeout，当src对应的列表中没有数据时，阻塞等待其有数据的超时时间（秒），0 表示永远阻塞 自定义增量迭代 123456789101112131415161718由于redis类库中没有提供对列表元素的增量迭代，如果想要循环name对应的列表的所有元素，那么就需要： 1、获取name对应的所有列表 2、循环列表但是，如果列表非常大，那么就有可能在第一步时就将程序的内容撑爆，所有有必要自定义一个增量迭代的功能： def list_iter(name): """ 自定义redis列表增量迭代 :param name: redis中的name，即：迭代name对应的列表 :return: yield 返回 列表元素 """ list_count = r.llen(name) for index in xrange(list_count): yield r.lindex(name, index) 使用for item in list_iter('pp'): print item Set操作Set集合就是不允许重复的列表 sadd(name,values) 1name对应的集合中添加元素 scard(name) 1获取name对应的集合中元素个数 sdiff(keys, *args) 1在第一个name对应的集合中且不在其他name对应的集合的元素集合 sdiffstore(dest, keys, *args) 1获取第一个name对应的集合中且不在其他name对应的集合，再将其新加入到dest对应的集合中 sinter(keys, *args) 1获取多一个name对应集合的并集 sinterstore(dest, keys, *args) 1获取多一个name对应集合的并集，再讲其加入到dest对应的集合中 sismember(name, value) 1检查value是否是name对应的集合的成员 smembers(name) 1获取name对应的集合的所有成员 smove(src, dst, value) 1将某个成员从一个集合中移动到另外一个集合 spop(name) 1从集合的右侧（尾部）移除一个成员，并将其返回 srandmember(name, numbers) 1从name对应的集合中随机获取 numbers 个元素 srem(name, values) 1在name对应的集合中删除某些值 sunion(keys, *args) 1获取多一个name对应的集合的并集 sunionstore(dest,keys, *args) 1获取多一个name对应的集合的并集，并将结果保存到dest对应的集合中 sscan(name, cursor=0, match=None, count=None) sscan_iter(name, match=None, count=None) 1同字符串的操作，用于增量迭代分批获取元素，避免内存消耗太大 有序集合在集合的基础上，为每元素排序；元素的排序需要根据另外一个值来进行比较，所以，对于有序集合，每一个元素有两个值，即：值和分数，分数专门用来做排序。 zadd(name,*args,**kwargs) 12345在name对应的有序集合中添加元素 如： zadd('zz', 'n1', 1, 'n2', 2) 或 zadd('zz', n1=11, n2=22) zcard(name) 1获取name对应的有序集合元素的数量 zcount(name, min, max) 1获取name对应的有序集合中分数 在 [min,max] 之间的个数 zincrby(name, value, amount) 1自增name对应的有序集合的 name 对应的分数 r.zrange( name, start, end, desc=False, withscores=False, score_cast_func=float) 123456789101112131415161718按照索引范围获取name对应的有序集合的元素 参数： name，redis的name start，有序集合索引起始位置（非分数） end，有序集合索引结束位置（非分数） desc，排序规则，默认按照分数从小到大排序 withscores，是否获取元素的分数，默认只获取元素的值 score_cast_func，对分数进行数据转换的函数 更多： 从大到小排序 zrevrange(name, start, end, withscores=False, score_cast_func=float) 按照分数范围获取name对应的有序集合的元素 zrangebyscore(name, min, max, start=None, num=None, withscores=False, score_cast_func=float) 从大到小排序 zrevrangebyscore(name, max, min, start=None, num=None, withscores=False, score_cast_func=float) zrank(name, value) 1234获取某个值在 name对应的有序集合中的排行（从 0 开始） 更多： # zrevrank(name, value)，从大到小排序 zrangebylex(name, min, max, start=None, num=None) 1234567891011121314151617当有序集合的所有成员都具有相同的分值时，有序集合的元素会根据成员的 值 （lexicographical ordering）来进行排序，而这个命令则可以返回给定的有序集合键 key 中， 元素的值介于 min 和 max 之间的成员对集合中的每个成员进行逐个字节的对比（byte-by-byte compare）， 并按照从低到高的顺序， 返回排序后的集合成员。 如果两个字符串有一部分内容是相同的话， 那么命令会认为较长的字符串比较短的字符串要大 参数： name，redis的name min，左区间（值）。 + 表示正无限； - 表示负无限； ( 表示开区间； [ 则表示闭区间 min，右区间（值） start，对结果进行分片处理，索引位置 num，对结果进行分片处理，索引后面的num个元素 如： # ZADD myzset 0 aa 0 ba 0 ca 0 da 0 ea 0 fa 0 ga # r.zrangebylex('myzset', "-", "[ca") 结果为：['aa', 'ba', 'ca'] 更多： 从大到小排序 zrevrangebylex(name, max, min, start=None, num=None) zrem(name, values) 12删除name对应的有序集合中值是values的成员如：zrem('zz', ['s1', 's2']) zremrangebyrank(name, min, max) 1根据排行范围删除 zremrangebyscore(name, min, max) 1根据分数范围删除 zremrangebylex(name, min, max) 1根据值返回删除 zscore(name, value) 1获取name对应有序集合中 value 对应的分数 zinterstore(dest, keys, aggregate=None) 12获取两个有序集合的交集，如果遇到相同值不同分数，则按照aggregate进行操作aggregate的值为: SUM MIN MAX zunionstore(dest, keys, aggregate=None) 12获取两个有序集合的并集，如果遇到相同值不同分数，则按照aggregate进行操作aggregate的值为: SUM MIN MAX zscan(name, cursor=0, match=None, count=None, score_cast_func=float) zscan_iter(name, match=None, count=None,score_cast_func=float) 1同字符串相似，相较于字符串新增score_cast_func，用来对分数进行操作 其他常用操作delete(*names) 1根据删除redis中的任意数据类型 exists(name) 1检测redis的name是否存在 keys(pattern=’*’) 1234567根据模型获取redis的name 更多： KEYS * 匹配数据库中所有 key 。 KEYS h?llo 匹配 hello ， hallo 和 hxllo 等。 KEYS h*llo 匹配 hllo 和 heeeeello 等。 KEYS h[ae]llo 匹配 hello 和 hallo ，但不匹配 hillo expire(name ,time) 1为某个redis的某个name设置超时时间 rename(src, dst) 1对redis的name重命名为 move(name, db)) 1将redis的某个值移动到指定的db下 randomkey() 1随机获取一个redis的name（不删除） type(name) 1获取name对应值的类型 scan(cursor=0, match=None, count=None) scan_iter(match=None, count=None) 1同字符串操作，用于增量迭代获取key 管道redis-py默认在执行每次请求都会创建（连接池申请连接）和断开（归还连接池）一次连接操作，如果想要在一次请求中指定多个命令，则可以使用pipline实现一次请求指定多个命令，并且默认情况下一次pipline 是原子性操作。 12345678910111213import redis pool = redis.ConnectionPool(host='10.211.55.4', port=6379) r = redis.Redis(connection_pool=pool) # pipe = r.pipeline(transaction=False)pipe = r.pipeline(transaction=True)pipe.multi()pipe.set('name', 'alex')pipe.set('role', 'sb') pipe.execute() 发布订阅12345678910111213141516171819import redisclass RedisHelper: def __init__(self): self.__conn = redis.Redis(host='10.211.55.4') self.chan_sub = 'fm104.5' self.chan_pub = 'fm104.5' def public(self, msg): self.__conn.publish(self.chan_pub, msg) return True def subscribe(self): pub = self.__conn.pubsub() pub.subscribe(self.chan_sub) pub.parse_response() return pub 订阅者 12345678from monitor.RedisHelper import RedisHelper obj = RedisHelper()redis_sub = obj.subscribe() while True: msg= redis_sub.parse_response() print msg 发布者： 1234from monitor.RedisHelper import RedisHelper obj = RedisHelper()obj.public('hello') sentinel(哨兵)redis重的sentinel主要用于在redis主从复制中，如果master顾上，则自动将slave替换成master 12345678910111213141516171819202122232425from redis.sentinel import Sentinel 连接哨兵服务器(主机名也可以用域名)sentinel = Sentinel([('10.211.55.20', 26379), ('10.211.55.20', 26380), ], socket_timeout=0.5) 获取主服务器地址 master = sentinel.discover_master('mymaster') print(master) 获取从服务器地址slave = sentinel.discover_slaves('mymaster') print(slave)获取主服务器进行写入 master = sentinel.master_for('mymaster') master.set('foo', 'bar') 获取从服务器进行读取（默认是round-roubin） slave = sentinel.slave_for('mymaster', password='redis_auth_pass') r_ret = slave.get('foo') print(r_ret)]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql进阶]]></title>
    <url>%2F2019%2F01%2F14%2Fmysql%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[存储过程一.存储过程的定义存储过程是把一段代码封装起来，当要执行这一段代码的时候，可以通过调用该存储过程来实现（经过第一次编译后再次调用不需要再次编译，比一个个执行sql语句效率高） 二.存储过程的优点 通常存储过程有助于提高应用程序的性能。当创建，存储过程被编译之后，就存储在数据库中。 但是，MySQL实现的存储过程略有不同。 MySQL存储过程按需编译。 在编译存储过程之后，MySQL将其放入缓存中。 MySQL为每个连接维护自己的存储过程高速缓存。 如果应用程序在单个连接中多次使用存储过程，则使用编译版本，否则存储过程的工作方式类似于查询。 存储过程有助于减少应用程序和数据库服务器之间的流量，因为应用程序不必发送多个冗长的SQL语句，而只能发送存储过程的名称和参数。 存储的程序对任何应用程序都是可重用的和透明的。 存储过程将数据库接口暴露给所有应用程序，以便开发人员不必开发存储过程中已支持的功能。 存储的程序是安全的。 数据库管理员可以向访问数据库中存储过程的应用程序授予适当的权限，而不向基础数据库表提供任何权限。 三.存储过程的缺点 如果使用大量存储过程，那么使用这些存储过程的每个连接的内存使用量将会大大增加。 此外，如果您在存储过程中过度使用大量逻辑操作，则CPU使用率也会增加，因为数据库服务器的设计不当于逻辑运算 存储过程的构造使得开发具有复杂业务逻辑的存储过程变得更加困难。 很难调试存储过程。只有少数数据库管理系统允许您调试存储过程。不幸的是，MySQL不提供调试存储过程的功能。 开发和维护存储过程并不容易。开发和维护存储过程通常需要一个不是所有应用程序开发人员拥有的专业技能。这可能会导致应用程序开发和维护阶段的问题。 四.一个简单的mysql存储过程示例123456delimiter // create procedure b1() begin select * from blog; end //delimiter ; 解释: 第一个命令是delimiter //，它与存储过程语法无关。 delimter语句将标准分隔符 - 分号(;)更改为：//。 在这种情况下，分隔符从分号(;)更改为双斜杠//。为什么我们必须更改分隔符？ 因为我们想将存储过程作为整体传递给服务器，而不是让mysql工具一次解释每个语句。 在END关键字之后，使用分隔符//来指示存储过程的结束。 最后一个命令(DELIMITER;)将分隔符更改回分号(;)。 .使用create procedure语句创建一个新的存储过程。在create procedure语句之后指定存储过程的名称。在这个示例中，存储过程的名称为：b1，并把括号放在存储过程的名字之后。 begin和end之间的部分称为存储过程的主体。将声明性SQL语句放在主体中以处理业务逻辑。 在这个存储过程中，我们使用一个简单的select语句来查询blog表中的数据。 123456mysql中调用存储过程 call b1()在python中基于pymysql调用cursor.callproc('b1') print(cursor.fetchall()) 五.声明变量要在存储过程中声明变量，可以使用delclare语句，如下 1DECLARE variable_name datatype(size) DEFAULT default_value; 在DECLARE关键字后面要指定变量名。变量名必须遵循MySQL表列名称的命名规则 指定变量的数据类型及其大小。变量可以有任何MySQL数据类型，如INT，VARCHAR，DATETIME等。 当声明一个变量时，它的初始值为NULL。但是可以使用DEFAULT关键字为变量分配默认值 1234567891011delimiter // create procedure b2() begin DECLARE n int DEFAULT 1; set n = 5; select * from blog where id = n; end //delimiter ;# mysql中调用存储过程call b2(); 六.存储过程传参在现实应用中，开发的存储过程几乎都需要参数。这些参数使存储过程更加灵活和有用。 在MySQL中，参数有三种模式：IN，OUT或INOUT。 IN - 是默认模式。在存储过程中定义IN参数时，调用程序必须将参数传递给存储过程。 另外，IN参数的值被保护。这意味着即使在存储过程中更改了IN参数的值，在存储过程结束后仍保留其原始值。换句话说，存储过程只使用IN参数的副本。 OUT - 可以在存储过程中更改OUT参数的值，并将其更改后新值传递回调用程序。请注意，存储过程在启动时无法访问OUT参数的初始值。 INOUT - INOUT参数是IN和OUT参数的组合。这意味着调用程序可以传递参数，并且存储过程可以修改INOUT参数并将新值传递回调用程序。 在存储过程中定义参数的语法如下 1MODE param_name param_type(param_size) 根据存储过程中参数的目的，MODE可以是IN，OUT或INOUT。 param_name是参数的名称。参数的名称必须遵循MySQL中列名的命名规则。 在参数名之后是它的数据类型和大小。和变量一样，参数的数据类型可以是任何有效的MySQL数据类型 如果存储过程有多个参数，则每个参数由逗号(,)分隔。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 1.indelimiter // create procedure b3( in blogName varchar(30) ) begin select * from blog where NAME = blogName; end //delimiter ;#mysql中调用存储过程call b3('第5篇');#python中调用存储过程cursor.callproc('b3',args = ('第5篇')); # 2.outdelimiter // create procedure b4( in year int, out count int ) begin SELECT COUNT(1) into count FROM blog GROUP BY DATE_FORMAT(sub_time,'%Y') having max(DATE_FORMAT(sub_time,'%Y')) = year ; set count = 6; end //delimiter ;call b4(2016,@count);select @count; #out只能当返回值 # 3.inoutdelimiter // create procedure b5( inout n1 int ) begin select * from blog where id &gt; n1; end //delimiter ;#mysql中调用set @n = 3;call b5(@n);select @n;#在python中基于pymysql调用cursor.callproc('b5',(4))print(cursor.fetchall()) #查询select的查询结果cursor.execute('select @n1') print(cursor.fetchall())# inout:既可以传入又可以返回 事务事务用于将某些操作的多个sql作为原子性操作,一旦有某一个出现错误,即可回滚到原来的状态,从而保证数据库的完整性 事务的四大特性 原子性:是指事务是一个不可分割的整体,事务中的操作要么就全部发生,要么都不成功 一致性:事务处理前后数据的完整性必须保持一致,完整性是指一个数据在某个时间点完全满足数据库中的约束要求 隔离性:是指多个用户访问一个数据库时,一个用户的事务处理不能被其他用户的事务所干扰,多个并发事务之间相互隔离 持久性:是指一个事务一旦被提交,他对数据库中的数据改变是永久的 举例说明 1234567891011121314151617181920212223242526create table user2(id int primary key auto_increment,name char(32),balance int);insert into user2(name,balance)values('wsb',1000),('egn',1000),('ysb',1000);#原子操作start transaction;update user2 set balance=900 where name='wsb'; #买支付100元update user2 set balance=1010 where name='egon'; #中介拿走10元update user2 set balance=1090 where name='ysb'; #卖家拿到90元commit;#出现异常，回滚到初始状态start transaction;update user2 set balance=900 where name='wsb'; #买支付100元update user2 set balance=1010 where name='egon'; #中介拿走10元uppdate user2 set balance=1090 where name='ysb'; #卖家拿到90元,出现异常没有拿到rollback; 下面是操作：当p_return_code为1时，表示异常，立马回滚。当为2时，出现警告，立马回滚原始状态。0表示成功 1234567891011121314151617181920212223242526272829303132delimiter //create PROCEDURE b6( OUT p_return_code tinyint)BEGIN DECLARE exit handler for sqlexception BEGIN -- ERROR set p_return_code = 1; rollback; END; DECLARE exit handler for sqlwarning BEGIN -- WARNING set p_return_code = 2; rollback; END; START TRANSACTION; insert into blog(name,sub_time) values('yyy',now()); COMMIT; -- SUCCESS set p_return_code = 0; #0代表执行成功END //delimiter ;set @res=123;call b6(@res);select @res; 索引一.索引的介绍数据库中专门用于帮助用户快速查找数据的一种数据结构。类似于字典中的目录，查找字典内容时可以根据目录查找到数据的存放位置吗，然后直接获取。 二.索引的作用约束和加速查找 三.常见的几种索引 普通索引 唯一索引 主键索引 联合索引 联合主键索引 联合唯一索引 联合普通索引 无索引： 从前往后一条一条查询 有索引：创建索引的本质，就是创建额外的文件（某种格式存储，查询的时候，先去格外的文件找，定好位置，然后再去原始表中直接查询。但是创建索引越多，会对硬盘也是有损耗。 建立索引的目的： a.额外的文件保存特殊的数据结构 b.查询快，但是插入更新删除依然慢 c.创建索引之后，必须命中索引才能有效 四.索引的种类hash索引和BTree索引 hash类型的索引：查询单条快，范围查询慢 btree类型的索引：b+树，层数越多，数据量指数级增长（我们就用它，因为innodb默认支持它） 五.索引的实现原理数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。 为表设置索引要付出代价的 一是增加了数据库的存储空间, 二是在插入和修改数据的时候要花费较多的时间(因为索引也要随之变动) 六.索引详细解释普通索引作用:仅有一个加速查找 1234567创建表+普通索引create table userinfo( nid int not null auto_increment primary key, name varchar(32) not null, email varchar(64) not null, index ix_name(name) ); 12普通索引create index 索引的名字 on 表名(列名) 12删除索引drop index 索引的名字 on 表名 12查看索引show index from 表名 唯一索引唯一索引的两个功能:加速查找和唯一约束(可含null) 1234567创建表和唯一索引create table userinfo( id int not null auto_increment primary key, name varchar(32) not null, email varchar(64) not null, unique index ix_name(name) ); 12唯一索引create unique index 索引名 on 表名(列名) 12删除唯一索引drop index 索引名 on 表名; 主键索引主键索引的两个功能:加速查找和唯一约束(可含null) 1234567891011121314151617 创建表和主键索引 create table userinfo( id int not null auto_increment primary key, name varchar(32) not null, email varchar(64) not null, unique index ix_name(name) ) orcreate table userinfo( id int not null auto_increment, name varchar(32) not null, email varchar(64) not null, primary key(nid), unique index ix_name(name) ) 12创建主键索引alter table 表名 add primary key(列名); 123删除主键索引alter table 表名 drop primary key;alter table 表名 modify 列名 int, drop primary key; 组合索引组合索引是将n个列组合成一个索引 12联合普通索引create index 索引名 on 表名(列名1,列名2); 七.索引名词1234覆盖索引:在索引文件中直接获取数据select name from userinfo where name = 'alex50000';索引合并:把多个单例索引合并使用select * from userinfo where name = 'alex13131' and id = 13131; 八.索引注意事项123456789(1)避免使用select *(2)count(1)或count(列) 代替count(*)(3)创建表时尽量使用char代替varchar(4)表的字段顺序固定长度的字段优先(5)组合索引代替多个单列索引（经常使用多个条件查询时）(6)尽量使用短索引 （create index ix_title on tb(title(16));特殊的数据类型 text类型）(7)使用连接（join）来代替子查询(8)连表时注意条件类型需一致(9)索引散列（重复少）不适用于建索引，例如：性别不合适 数据库的引擎mysql所支持的引擎123show engines\G;查看所有支持的引擎show variables like 'storage_engine%'; 查看正在使用的存储引擎create table t1(id int)engine=innodb;# 指定表类型/存储引擎 默认不写就是innodb 1.innoDB存储引擎支持事务,其设计目标主要面向联机事务处理(OLTP)的应用,其特点是行锁的设计,支持外键,并支持类似oracle的非锁定读,即默认读取操作不会产生锁,从mysql5.58版本开始是默认的存储引擎 2.MylSAM存储引擎不支持事务,表锁设计,支持全文索引,主要面向一些OLAP数据库应用,在mysql5.58版本之前是默认的存储引擎,(除 Windows 版本外 )数据库系统 与文件系统一个很大的不同在于对事务的支持,MyISAM 存储引擎是不支持事务的。究其根 本,这也并不难理解。用户在所有的应用中是否都需要事务呢?在数据仓库中,如果没有 ETL 这些操作,只是简单地通过报表查询还需要事务的支持吗?此外,MyISAM 存储引擎的 另一个与众不同的地方是,它的缓冲池只缓存(cache)索引文件,而不缓存数据文件,这与 大多数的数据库都不相同。 3.NDB存储引擎 NDB 存储引擎是一个集群存储引擎,类似于 Oracle 的 RAC 集群,不过与 Oracle RAC 的 share everything 结构不同的是,其结构是 share nothing 的集群架构,因此能提供更高级别的 高可用性。NDB 存储引擎的特点是数据全部放在内存中(从 5.1 版本开始,可以将非索引数 据放在磁盘上),因此主键查找(primary key lookups)的速度极快,并且能够在线添加 NDB 数据存储节点(data node)以便线性地提高数据库性能。由此可见,NDB 存储引擎是高可用、 高性能、高可扩展性的数据库集群系统,其面向的也是 OLTP 的数据库应用类型。 4、Memory 存储引擎正如其名,Memory 存储引擎中的数据都存放在内存中,数据库重 启或发生崩溃,表中的数据都将消失。它非常适合于存储 OLTP 数据库应用中临时数据的临时表,也可以作为 OLAP 数据库应用中数据仓库的维度表。Memory 存储引擎默认使用哈希 索引,而不是通常熟悉的 B+ 树索引。 5.Infobright 存储引擎第三方的存储引擎。其特点是存储是按照列而非行的,因此非常 适合 OLAP 的数据库应用。其官方网站是 http://www.infobright.org/,上面有不少成功的数据 仓库案例可供分析。 6、NTSE 存储引擎网易公司开发的面向其内部使用的存储引擎。目前的版本不支持事务, 但提供压缩、行级缓存等特性,不久的将来会实现面向内存的事务支持 7、BLACKHOLE黑洞存储引擎，可以应用于主备复制中的分发主库。 数据库的锁表级别锁(table-level)表级别的锁定是mysql个存储引擎中最大颗粒度的锁定机制,该锁定最大的特点就是实现逻辑非常简单,带来的系统负面影响最小,所以获取锁和释放锁都非常快,由于表级锁一次会将整个表都锁住,所以可以很好的避免死锁的问题 使用表级锁的主要是:myiSAM,MEMORY,CSV等一些非事务型存储引擎 行级锁(row-level)行级锁最大的特点就是锁定对象的颗粒度很小,也是目前各大数据库管理软件所实现的锁定颗粒度最小的,由于颗粒度很小,所以发生多锁定字段争用的概率也是最小的,能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能 虽然在并发处理上有很大的优势,但是行级索引也因此带来很多的弊端,由于锁定资源的颗粒度很小,所以每次获取和释放锁需要做的事情也多了,因此带来的消耗也就大了,行级锁很容易带来死锁 使用行级锁的主要是innoDB存储引擎 页级锁定(page-level)页级锁定是mysql中比较独特的一种锁定级别,在其他的数据库管理中不是太常见,页级锁定的特点:锁定颗粒度介于行级锁和表级锁之间的,所以获取锁定所需要的开销以及所能够提供的并发处理能力也是介于上面两者之间的, 使用页级锁定的主要是berkeleyDB存储引擎 总结表级锁:开销小.加锁快,不会出现死锁;锁定粒度大,发生锁冲突的概率最高,并发度最低 行级锁:开销大,加锁慢,会出现死锁,锁定粒度最小,发生锁冲突概率最低,并发程度最高 页面锁:开锁和加锁时间介于表锁和行锁之间,会出现死锁;锁定粒度介于表锁和行锁之间,并发一般 应用三个锁之间各有各的特点,如果从锁的角度来说,表级锁更适合查询为主,只有少量按索引条件更新数据的应用 ,如web应用;航迹锁更适用于有大量按索引条件,并发更新少量不同数据,同时又有并发查询的应用,如一些在线事务处理(OLTP)系统 MYSQL表级锁有两种模式 表共享读锁(Table Read Lock) 对mylSAM表进行读取操作时不会阻塞其他用户对同一表的写操作 表独占写锁(Table write Lock) 对MylSAM表的写操作,则会阻塞对其他用户对同一表的读写操作 innoDB和MyiSAM锁最大的不同 InnoDB支持事务,Myisam不支持 innoDB可以使用行级锁和表锁,MyiSAM只支持表锁 视图1.视图的定义视图是虚拟表或逻辑表，它被定义为具有连接的SQL SELECT查询语句。因为数据库视图与数据库表类似，它由行和列组成，因此可以根据数据库表查询数据。其内容由查询定义。 但是，视图并不在数据库中以存储的数据值集形式存在，行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。简单的来说视图是由其定义结果组成的表； 2.视图的优点1.数据库视图允许简化复杂查询，通过数据库视图，您只需使用简单的SQL语句，而不是使用具有多个连接的复杂的SQL语句。2.安全性。一般是这样做的:创建一个视图，定义好该视图所操作的数据。之后将用户权限与视图绑定。这样的方式是使用到了一个特性：grant语句可以针对视图进行授予权限。 3.视图的缺点1、性能：从数据库视图查询数据可能会很慢，特别是如果视图是基于其他视图创建的。 2、表依赖关系：将根据数据库的基础表创建一个视图。每当更改与其相关联的表的结构时，都必须更改视图。 4.创建视图语法: create view 视图名称 as sql语句 12create view teacher_view as select tid from teacher where tname='李平老师';select cname from course where teacher_id = (select tid from teacher_view); 5.使用视图12345往真实表中插入一条数据，查看一下视图，发现视图表也会跟着更新insert into course(cname,teacher_id) values('张三丰',2);更新一下数据，发现视图的数据也会跟着更新update course set cname='王五';不能修改视图的数据 6.修改视图12语法：ALTER VIEW 视图名称 AS SQL语句alter view teacher_view as select * from course where cid&gt;3; 7.删除视图12语法：DROP VIEW 视图名称DROP VIEW teacher_view 触发器触发器:触发器是一个特殊的存储过程，它是MySQL在insert、update、delete的时候自动执行的代码块。 1.创建触发器1234567891011121314151617181920212223242526272829303132333435# 插入前CREATE TRIGGER tri_before_insert_tb1 BEFORE INSERT ON tb1 FOR EACH ROWBEGIN ...END# 插入后CREATE TRIGGER tri_after_insert_tb1 AFTER INSERT ON tb1 FOR EACH ROWBEGIN ...END# 删除前CREATE TRIGGER tri_before_delete_tb1 BEFORE DELETE ON tb1 FOR EACH ROWBEGIN ...END# 删除后CREATE TRIGGER tri_after_delete_tb1 AFTER DELETE ON tb1 FOR EACH ROWBEGIN ...END# 更新前CREATE TRIGGER tri_before_update_tb1 BEFORE UPDATE ON tb1 FOR EACH ROWBEGIN ...END# 更新后CREATE TRIGGER tri_after_update_tb1 AFTER UPDATE ON tb1 FOR EACH ROWBEGIN ...END 1234567891011121314151617181920212223242526272829# 创建用户表create table user( id int primary key auto_increment, name varchar(20) not null, reg_time datetime, # 注册用户的时间 affirm enum('yes','no') # no表示该用户执行失败);#创建日志表create table userLog( id int primary key auto_increment, u_name varchar(20) not null, u_reg_time datetime # 注册用户的时间);# 创建触发器 delimiter 默认情况下，delimiter是分号 触发器名称应遵循命名约定[trigger time]_[table name]_[trigger event]delimiter //create trigger after_user_insert after insert on user for each rowbegin if new.affirm = 'yes' then insert into userLog(u_name,u_reg_time) values(new.name,new.reg_time); end if;end //delimiter ;#往用户表中插入记录，触发触发器，根据if的条件决定是否插入数据insert into user(name,reg_time,affirm) values ('张三',now(),'yes'),('李四',now(),'yes'),('王五',now(),'no'); 注意:请注意，在为INSERT定义的触发器中，可以仅使用NEW关键字。不能使用OLD关键字。但是，在为DELETE定义的触发器中，没有新行，因此您只能使用OLD关键字。在UPDATE触发器中，OLD是指更新前的行，而NEW是更新后的行 2.使用触发器触发器无法由用户直接调用,而只能由于对表的[增删改查]操作被动引起的 3.删除触发器1drop trigger trigger_userLog;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql基础]]></title>
    <url>%2F2019%2F01%2F14%2Fmysql%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[mysql数据库基本命令数据库的增删改查12345678910111213-- 增create database 数据库名称;-- 删drop database 库名;-- 改alter database 库名 更改的内容;-- 查show create database 库名;查看当前创建的数据库show databases; 查看所有的数据库select database();查看所在的数据库 表的增删改查1234567891011121314-- 增create table 表名(字段名 类型 约束);-- 删drop table 表名;-- 改alter table 表名 modify 字段名 类型;alter table 表名 add 字段名;-- 查show create table 表名;查看表的详细信息show tables;查看这个数据库先所有的表desc 表名;查看这个表里面所有的字段 数据的增删改查123456789101112131415-- 增insert into 表名(字段名) values (内容);-- 删delete from 表名; 清空数据,但是里面的id自增不会去除truncate 表名; 删除所有的内容-- 改update 表名 set name="xxx" where 条件update 表名 set 要修改的内容-- 查select 字段名 from 表名;select 字段1,字段2 from 表名;select * from 表名; 查看所有的内容 mysql的数据类型数据类型:定义列中可以存储什么数据以及该数据实际怎样存储的基本规则，其用于以下几个目的 允许限制可存储在列中的数据 允许在内部更有效的存储数据 允许变换排序顺序（作为数值数据类型，数值才能正确排序） 一.字符串数据类型该类型为最常用的数据类型，用来存储串（比如名字、地址等）；有两种串类型，分别是定长串和变长串 定长串：接受长度固定的字符串，其长度实在创建表时指定的；定长列不允许多余指定的字符数目，它们分配的存储空间与指定的一样多（比如char） 变长串：存储可变长度的文本，有些变长数据类型具有最大定长，有些是完全变长的，不论哪种，指定的数据得到保存即可（灵活） 数据类型 大小 用途 char 0-255字节 定长字符串 varchar 0-65535字节 变长字符串 tinyblog 0-255字节 不超过255个字符的二进制字符串 tinytext 0-255字节 短文本字符串 blob 0-65535字节 二进制形式的长文本数据 text 0-65535字节 长文本数据 mediumblob 0-16777215字节 二进制形式的中等长度文本数据 mediumtext 0-16777215字节 中等长度文本数据 二.数值类型 类型 大小 范围(有符号) 无符号 用途 tinyint 1字节 (-128,127) (0,255) 小整数值 smallint 2字节 (-32768,32767) (0,35535) 大整数值 mediumint 3字节 (-8 388 608，8 388 607) (0，16 777 215) 大整数值 int或integer 4字节 (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 bigint 8 字节 (-9 233 372 036 854 775 808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 float 4字节 (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 double 8字节 (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 decimal 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 三.日期和时间类型表示时间值的日期和时间类型为DATETIME、DATE、TIMESTAMP、TIME和YEAR。 类型 大小(字节) 范围 格式 用途 date 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 time 3 -838:59:59’/‘838:59:59’ HH:MM:SS 时间值或持续时间 year 1 1901/2155 YYYY 年份值 datetime 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 timestamp 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 四.枚举和集合 enum 单选只能在给定的范围内选一个值,如sex男和male女 set 多选 在给定的范围内可以选择一个或一个以上的值(爱好1,爱好2) 完整性约束 类型 说明 not null 不能为空 default 默认值 unique 设置唯一 primary key 主键 auto_increment 自增 foreign key 外键 unsigned 无符号 zerofill 使用0填充 on delete cascade 同步删除 on update cascade 同步更新 表查询单表查询 from: 找表 where :指定约束条件 比较运算符:&gt;,&lt;,&gt;=,&lt;=,!= between 80 and 100 :值在80到100之间 in(80,90,100):值是80,90或100 like’xiaomage‘:模糊查找也可以是%,或_:模糊查找 逻辑运算符:and or not group by:分组 having:过滤 执行优先级:where&gt;group by &gt;having 1.where 发生在分组 group by之前,因而where 中可以有任意字段,但绝对不能使用聚合函数 2.having 发生在分组 group by之后,因而having中可以使用分组字段,无法直接取到其他字段,可以使用其他函数 select :挑选 distinct:去重 order by:排序 asc:正序 小–&gt;大 DESC:倒序 大—-&gt;小 limit:限制结果的显示条数 第一个是:起始位置 第二个是:显示的条数 多表查询 select 字段 from 表1 inner | left |right join 表2 on 表1.字段 = 表2.字段 inner 是只连接匹配的行 right:外键之右连接:优先显示右半部分 left:外键之左连接:优先显示左半部分 union:连接left和right:可以显示全部信息 符合条件的连接查询 on:后面加两个表的比较条件 子查询 子查询是将一个查询语句嵌套在另一个查询语句中 内层查询语句的查询结果,可以为外层查询语句提供条件 子查询中可以包含:in ,not,any,all,exists和not exists EXISTS:判断的是真和假,有内容为真,没有就是假 聚合函数 函数名 说明 max() 求最大值 min() 求最小值 avg() 求平均值 sum() 求和 count() 求总个数 group_concat(name): 查找这个组里所有的名字 pymsql的使用(python)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import pymysql1.连接 host:数据库地址 port:端口 user:用户名 password:密码 db:数据库 charset:指定类型conn = pymysql.connect(host='127.0.0.1', port=3306, user='root', password='', db='db8', charset='utf8') 2.创建游标cursor = conn.cursor()# 防止注入sql = "select * from userinfo where username='%s' and pwd='%s'" cursor.execute(sql,[user,pwd])# 增# 一条sql = "insert into userinfo(username,pwd,) values(xx,oo)" cursor.execute(sql)# 多条cursor.executemany(sql,[(),(),()])# 删sql = "delete from userinfo where id = 2"cursor.execute(sql)# 改sql = "update userinfo set userinfo=%s where id = 2"cursor.execute(sql,username)# 查fetchone():获取下一行数据，第一次为首行；fetchall():获取所有行数据源fetchmany(4):获取4行数据sql = 'select * from userinfo'cursor.execute(sql)# 查询第一行的数据row = cursor.fetchone() # 获取查询的指定条数的信息ret = cursor.fetchmany(3)# 获取查询的所有信息ret = cursor.fetchall()# ------------------事务--------------sql = "insert into userinfo(name,password) values(%s,%s)"try: # 执行SQL语句 res = cursor.execute(sql,["rain222","1234"]) # 提交事务 conn.commit() # 提交之后，获取刚插入的数据的ID last_id = cursor.lastrowidexcept Exception as e: # 有异常，回滚事务 conn.rollback()# ----------------------------------------# 移动指针cursor.scroll(1,mode='relative') # 相对当前位置移动cursor.scroll(2,mode='absolute') # 相对绝对位置移动第一个值为移动的行数，整数为向下移动，负数为向上移动，mode指定了是相对当前位置移动，还是相对于首行移动# 关闭连接，游标和连接都要关闭cursor.close()conn.close() 数据库的备份和还原备份1234567891011-- 备份一个数据库基本语法mysqldump -u username -p dbname table1 table2 ...-&gt; BackupName.sql dbname参数表示数据库的名称； table1和table2参数表示需要备份的表的名称，为空则整个数据库备份； BackupName.sql参数表设计备份文件的名称，文件名前面可以加上一个绝对路径。通常将数据库被分成一个后缀名为sql的文件；-- 备份多个数据库mysqldump -u username -p --databases dbname2 dbname2 &gt; Backup.sql 加上了--databases选项，然后后面跟多个数据库-- 备份所有数据库mysqldump -u -root -p -all-databases &gt; D:\all.sql 还原1mysql -u root -p &lt; C:\backup.sql 注意：这种方法不适用于InnoDB存储引擎的表，而对于MyISAM存储引擎的表很方便。同时，还原时MySQL的版本最好相同。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git学习笔记]]></title>
    <url>%2F2019%2F01%2F14%2Fgit%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[安装git12git config --global user.name "Your Name"git config --global user.email "email@example.com" 管理工作目录1234git init 初始化git status 查看工作树状态git log--oneline 查看提交记录git reflog 查看之前所有的操作记录 三种状态/区域的切换12345678工作区 暂存区 本地仓库git add ./&lt;filename&gt; 工作区到暂存区git commit -m "理由" 暂存区到本地仓库git checkout ./&lt;filename&gt; 丢弃工作区的内容git reset ./&lt;filename&gt; 从暂存区回退到工作区git reset --hard &lt;commit_id&gt; 版本的回退,并删除git reset --soft &lt;commit_id&gt; 把内容存到暂存区git reset &lt;commit_id&gt; 把内容回退到工作区 分支管理12345git branch &lt;name&gt; 创建分支git branch :查看分支git branch -d &lt;name&gt; 删除分支git checkout &lt;name&gt; 切换分支git merge &lt;name&gt; 把name分支合并到当前分支 标签管理123git tag &lt;name&gt; &lt;commit_id&gt; 给指定的版本加标签git tag 查看所有标签git tag -d &lt;name&gt; 删除标签 远程仓库建立连接123--ssh1.本地生成公钥私钥,在主用户的根目录下2.把公钥放入github中 和远程仓库建立连接1git remote add '远程仓库的别名' 远程仓库的地址 查看所有的仓库1git remote 向远程仓库提交代码12git push -u 远程仓库别名 分支名注意 -u 第一次提交代码的时候本地分支跟远程仓库的分支建立起连接 从远程仓库拉代码1git pull 远程仓库的别名 分支名 克隆项目1git clone 地址]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go基础部分]]></title>
    <url>%2F2019%2F01%2F14%2Fgo%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86%2F</url>
    <content type="text"></content>
      <categories>
        <category>go语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ的基本使用]]></title>
    <url>%2F2019%2F01%2F14%2FRabbitMQ%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[官网:RabbitMQ RabbitMQ是一个在AMQP基础上完整的，可复用的企业消息系统。他遵循Mozilla Public License开源协议 MQ全称为Message Queue, 消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过读写出入队列的消息（针对应用程序的数据）来通信，而无需专用连接来链接它们。消 息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信，直接调用通常是用于诸如远程过程调用的技术。排队指的是应用程序通过 队列来通信。队列的使用除去了接收和发送应用程序同时执行的要求。 RabbitMQ 安装1234567891011# 安装配置epel源rpm -ivh http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm# 安装erlangyum -y install erlang# 安装RabbitMQyum -y install rabbitmq-server# 开启和关闭RabbitMQservice rabbitmq-server start/stop 安装API 12345# python3安装pip install pika# python2安装easy_install pik 生产者消费者生产者123456789101112131415161718192021import pika# 无密码# connection = pika.BlockingConnection(pika.ConnectionParameters('IP地址'))# 有密码credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 声明一个队列(创建一个队列) 并支持持久化channel.queue_declare(queue='队列名称',durable=True)channel.basic_publish(exchange='', routing_key='队列名称', # 消息队列名称 body='发送的内容', properties=pika.BasicProperties( delivery_mode = 2, # 对信息进行持久化 )) # 关闭connection.close() 消费者12345678910111213141516171819202122import pika# 连接rabbitMQcredentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 声明一个队列(创建一个队列)channel.queue_declare(queue='队列名称')# 设置闲置时消费,那个消费者消费完了就接收任务channel.basic_qos(prefetch_count=1)# 设置回调函数def callback(ch, method, properties, body): print("消费者接受到了任务: %r" % body) ch.basic_ack(delivery_tag=method.delivery_tag) # 给服务端发送我接收到了 # 监听队列 no_ack=false 表示给服务端发送我接收到了的消息channel.basic_consume(callback,queue='队列名称',no_ack=False)# 开始监听channel.start_consuming() 发布者和订阅者全部订阅用户发布者 123456789101112131415import pika# 连接credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 设置 中间商 exchange_type : fanout(全部) channel.exchange_declare(exchange='中间商名称',exchange_type='fanout')# 向队列添加内容channel.basic_publish(exchange='中间商名称', routing_key='', body='内容')connection.close() 订阅者 123456789101112131415161718192021222324252627import pika# 连接credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 创建中间商(有的话就不会创建)channel.exchange_declare(exchange='中间商名称',exchange_type='fanout')# 随机生成一个队列result = channel.queue_declare(exclusive=True)queue_name = result.method.queu# 让exchange和queque进行绑定.channel.queue_bind(exchange='中间商名称',queue=queue_name)# 设置回调函数def callback(ch, method, properties, body): print("消费者接受到了任务: %r" % body)# 对队列进行监听channel.basic_consume(callback,queue=queue_name,no_ack=True)# 开始监听channel.start_consuming() 关键字发布发布者 12345678910111213141516import pika# 连接credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 创建中间商 exchange_type direct:代表关键字队列channel.exchange_declare(exchange='中间商名称',exchange_type='direct')channel.basic_publish(exchange='中间商名称', routing_key='关键字', body='内容')# 关闭connection.close() 订阅者 123456789101112131415161718192021222324252627import pika# 连接credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 创建中间商 exchange_type direct:代表关键字队列channel.exchange_declare(exchange='名称',exchange_type='direct')# 随机生成一个队列result = channel.queue_declare(exclusive=True)queue_name = result.method.queue# 让exchange和queque进行绑定. routing_key:指定关键字,可以绑定多次channel.queue_bind(exchange='名称',queue=queue_name,routing_key='关键字1')channel.queue_bind(exchange='名称',queue=queue_name,routing_key='关键字1')# 设置回调函数def callback(ch, method, properties, body): print("消费者接受到了任务: %r" % body)# 对队列进行监听channel.basic_consume(callback,queue=queue_name,no_ack=True)# 开始监听channel.start_consuming() 模糊匹配发布者 1234567891011121314151617import pika# 连接credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 创建一个中间商 topic:代表模糊匹配channel.exchange_declare(exchange='名称',exchange_type='topic')# 发送数据channel.basic_publish(exchange='m3', routing_key='xxx.xxx.py', body='内容')# 关闭connection.close() 订阅者 1234567891011121314151617181920212223242526import pika# 连接credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 创建一个中间商 topic:代表模糊匹配channel.exchange_declare(exchange='名称',exchange_type='topic')# 随机生成一个队列result = channel.queue_declare(exclusive=True)queue_name = result.method.queue# 让exchange和queque进行绑定. # :代表全部 *代表一个单词channel.queue_bind(exchange='名称',queue=queue_name,routing_key='xxx.#')# 回调函数def callback(ch, method, properties, body): print("消费者接受到了任务: %r" % body)# 监听channel.basic_consume(callback,queue=queue_name,no_ack=True)# 开始监听channel.start_consuming() 基于RabbitMQ事项RPC这是程序与程序之间的信息传递,所以起名中间商和服务商 中间商123456789101112131415161718192021222324252627282930313233343536373839404142434445import pikaimport uuidclass FibonacciRpcClient(object): # 连接rabbitMQ def __init__(self): credentials = pika.PlainCredentials("root", "123") self.connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14', credentials=credentials)) self.channel = self.connection.channel() # 随机生成一个消息队列(用于接收结果) result = self.channel.queue_declare(exclusive=True) self.callback_queue = result.method.queue # 监听消息队列中是否有值返回,如果有值则执行 on_response 函数(一旦有结果,则执行on_response) self.channel.basic_consume(self.on_response, no_ack=True,queue=self.callback_queue) def on_response(self, ch, method, props, body): if self.corr_id == props.correlation_id: self.response = body def call(self, n): self.response = None self.corr_id = str(uuid.uuid4()) # 中间商 给 服务商 发送一个任务: 任务id = corr_id / 任务内容 = '30' / 用于接收结果的队列名称 self.channel.basic_publish(exchange='', routing_key='rpc_queue', # 服务商接收任务的队列名称 properties=pika.BasicProperties( reply_to = self.callback_queue, # 用于接收结果的队列 correlation_id = self.corr_id, # 任务ID ), body=str(n)) # 循环监听信息有没有传回来 while self.response is None: # process_data_events也是一直监听有没有值 self.connection.process_data_events() return self.response# 实例化fibonacci_rpc = FibonacciRpcClient()# 调用程序response = fibonacci_rpc.call(50)print('返回结果:',response) 服务商12345678910111213141516171819202122232425262728293031import pika# 连接credentials = pika.PlainCredentials("root","123")connection = pika.BlockingConnection(pika.ConnectionParameters('192.168.19.14',credentials=credentials))channel = connection.channel()# 服务商监听任务队列channel.queue_declare(queue='rpc_queue')# 回调函数def on_request(ch, method, props, body): # 接收到内容并处理 n = int(body) response = n + 100 # 把处理好的信息通过队列把它返回回去 ch.basic_publish(exchange='', routing_key=props.reply_to, # 指定队列名称 properties=pika.BasicProperties(correlation_id= props.correlation_id), # 返回id body=str(response)) # body返回的是内容 # 返回一个ack 给服务端发送我接收到了 ch.basic_ack(delivery_tag=method.delivery_tag)# 设置闲置时消费,那个消费者消费完了就接收任务channel.basic_qos(prefetch_count=1)# 监听队列channel.basic_consume(on_request, queue='rpc_queue')# 开始监听channel.start_consuming()]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker的常用命令]]></title>
    <url>%2F2019%2F01%2F13%2Fdocker%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1.查看docker信息(version,info)12345# 查看版本docker version# 显示docker系统的信息docker info 2.对image的操作(search,pull,images,rmi,history) view plaincopy1234567891011121314# 检索imagedocker search image_name# 下载imagedocker pull image_name# 列出镜像列表:-a, --all=false Show all images; --no-trunc=false Don't truncate output; -q, --quiet=false Only show numeric IDs docker images# 删除一个或多个镜像 -f, --force=false Force; --no-prune=false Do not delete untagged parents docker rmi image_name # 显示一个镜像的历史 --no-trunc=false Don't truncate output; -q, --quiet=false Only show numeric IDs docker history image_name 3.启动容器(run)​ docker容器可以理解为在沙盒中运行的进程,这个沙盒包含该进程所必须的资源,包括文件系统,系统类库,shell环境等,但这个沙盒默认是不会运行任何程序的,你需要在沙盒中运行一个进程来启动某个容器,这个进程是该容器的唯一进程,所以当该进程结束时,容器也会结束 12345678# 在容器中运行"echo" 命令,输出"hello word"docker run image_name echo "hello word"# 交互式进入容器中docker run -i -t image_name /bin/bash# 在容器中安装新的程序docker run image_name apt-get install -y app_name ​ 在执行apt-get 命令的时候，要带上-y参数。如果不指定-y参数的话，apt-get命令会进入交互模式，需要用户输入命令来进行确认，但在docker环境中是无法响应这种交互的。apt-get 命令执行完毕之后，容器就会停止，但对容器的改动不会丢失。 4.查看容器(ps) view plaincopy12345678# 列出当前所有运行的containerdocker ps# 列出所有的containerdocker ps -a# 列出最近一次启动的containerdocker ps -1 5.保存对容器的修改(commit)​ 当你对一个容器进行修改之后(通过容器中运行某一个命令),可以把容器的修改保存下来,这样下一次可以从保存后的最新状态运行该容器 view plaincopy 12# 保存对容器的修改 -a, --author="" Author; -m, --message="" Commit message docker commit ID new_image_name ​ Note:image相当于一个类,container相当于实例，不过可以动态给实例安装新软件，然后把这个container用commit命令固化成一个image。 6.对容器的操作(rm、stop、start、kill、logs、diff、top、cp、restart、attach ) view plaincopy1234567891011121314151617181920212223242526272829# 删除所有容器docker rm `docker ps -a -q`# 删除单个容器docker rm Name/ID# 停止,启动,杀死一个容器docker stop Name/IDdocker start Name/IDdocker kill Name/ID# 从一个容器中取日志docker logs Name/ID# 列出一个容器里面被改变的文件或者目录,list列表会显示三种事件,A 增加 D删除 C被改变docker diff Name/ID# 显示一个运行的容器里面的进程信息docker top Name/ID# 从容器里面拷贝文件/目录到本地一个路径docker cp Name:/container_path to_path docker cp ID:/container_path to_path # 重启一个正在运行的容器docker restart Name/ID# 附加到一个运行的容器上docker attach ID ​ Note:attach命令允许你查看或影响一个运行的容器,你可以在同一时间attach同一个容器,你也可以从一个容器中脱离出来,是CTRL + C 7.保存和加载镜像(save load)​ 当需要把一台机器上的镜像迁移到另一台机器上的时候,需要保存于加载镜像 1234567891011# 保存镜像到一个tar包 -o, --output="" Write to an file docker save image_name -o file_path# 加载一个tar包格式的镜像 -i, --input="" Read from a tar archive file docker load -i file_path# 机器adocker save image_name &gt; /home/save.tar# 使用scp将save.tar拷到机器b上,然后docker load &lt; /home/save.tar 8.登录 registry server (login) view plaincopy12# 登录register server -e, --email="" Email; -p, --password="" Password; -u, --username="" Username docker login 9.发布image(push) copy12# 发布docker镜像docker push new_image_name 10.根据Dockerfile构建一个容器1#]]></content>
      <categories>
        <category>运维开发之路</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker的安装]]></title>
    <url>%2F2019%2F01%2F13%2Fdocker%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[docker安装1.Ubuntu系统Ubuntu安装docker大概要区分为Ubuntu14.04之前和Ubuntu14.04之后两种方法 如果是14.04版本之后的Ubuntu,其内核版本以及一些依赖包都已经准备充分,直接运行下载最新版docker即可: 1curl -sSL https://get.docker.com/ | sh 新安装的系统可能会没有curl服务,需要下载: 1sudo apt-get update $ sudo apt-get install curl 顺便提及,docker应用的启动需要root的管理员权限,最好在安装之前获取root权限,啰嗦一下如何方便地将用户转为root角色 1sudo su 然后根据提示输入当前用户密码即可. 下载好之后可以测试,下载hello-world或者busybox测试一下. 1sudo docker run hello-world docker run是docker的运行命令.后面是容器名称,如果本地没有该命令,则docker服务会从docker仓库下载该容器,然后运行. 测试打印 hello world就说明成功了.可用docker info查看安装信息. 最好使用新版本的Ubuntu安装docker.如果是12.04或者13.04版本的则需要先安装一些依赖性的包 先要升级内核(同样先获取root权限) 12sudo apt-get updatesudo apt-get install linux-image-generic-lts-raring linux-headers-generic-lts-raring Docker有deb格式的安装包 1sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9 后把Docker的库添加到apt的源列表中，更新并安装lxc-docker包。 1234sudo sh -c "echo deb http://get.docker.io/ubuntu docker main\&gt; /etc/apt/sources.list.d/docker.list"sudo apt-get updatesudo apt-get install lxc-docker 如果有警告信息,yes即可 2.centos系统和rhel这两个系统在新的版本里面都自带了docker,只不过docker版本不一定是最新的,我记得centos7里面的自带的docker是0.9,当前最新docker版本已经到了0.11,不过不会影响试用. 系统安装需要保证内核版本在3.10以上,低于这个版本的理论上也可以安装,只不过需要大牛去研究一番,我们直接升级内核 yum安装带aufs模块的3.10内核 123cd /etc/yum.repos.d wget http://www.hop5.in/yum/el6/hop5.repoyum install kernel-ml-aufs kernel-ml-aufs-devel 修改grub的主配置文件/etc/grub.conf，设置default=0，表示第一个title下的内容为默认启动的kernel（一般新安装的内核在第一个位置）,之后重启. 执行安装: 1curl -sSL https://get.docker.com/ | sh 启动服务: 1sudo service docker start 如果是系统版本7以上,已经自带docker包,直接运行: 1yum install docker]]></content>
      <categories>
        <category>运维开发之路</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible的基本使用]]></title>
    <url>%2F2019%2F01%2F13%2Fansible%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一.简介ansible是什么东西?官方的title是“Ansible is Simple IT Automation”——简单的自动化IT工具。这个工具的目标有这么几项：让我们自动化部署APP；自动化管理配置项；自动化的持续交付；自动化的（AWS）云服务管理。 所有的这几个目标本质上来说都是在一个台或者几台服务器上，执行一系列的命令而已。就像我之前有介绍过的Fabric，以及我们基于Fabric开发的自动化应用部署的工具： Essay 。都是做了这么个事——批量的在远程服务器上执行命令 。 那么fabric和ansible有什么差别呢？简单来说fabric像是一个工具箱，提供了很多好用的工具，用来在Remote执行命令，而Ansible则是提供了一套简单的流程，你要按照它的流程来做，就能轻松完成任务。这就像是库和框架的关系一样。 当然，它们之间也是有共同点的——都是基于 paramiko 开发的。这个paramiko是什么呢？它是一个纯Python实现的ssh协议库。因此fabric和ansible还有一个共同点就是不需要在远程主机上安装client/agents，因为它们是基于ssh来和远程主机通讯的。 二.安装及初步使用 编译安装,此处使用yum 12yum install epel # 下载epel源yum install -y ansible # 安装ansible 2.查看ansible生成的文件 1rpm -ql ansible 3.查看ansible生成的命令 1234ansible # 用来执行ansible的一些命令ansible-doc # 用来查看ansible的模块的帮助信息ansible-playbook # 用来执行playbookansible-galaxy # 用来下载第三方的playbook 4.ansible命令模式 1234567ansible &lt;host-pattern&gt; [options]-a MODULE_ARGS # 模块的参数-C --check # 测试,干跑-f FORKS # 指定并发数--list-hosts # 列出host-pattern主机--syntax-check # 语法检查-m MODULE_NAME # 指定模块 5.ansible第一条命令 1ansible all -m ping # 跟系统自带的ping不一样 6.host-pattern格式 123456789101112131415[web]192.168.19.33192.168.19.44[db]192.168.19.55[cache]192.168.19.66www[001:006].example.com指定所有 all指定单台机器(指定多个机器)指定分组(多个分组)指定分组并集 # ansible "web:db" -m ping指定分组的交集 # ansible "web:&amp;db" -m ping指定分组的差集 # ansible "web:!db" -m ping 7.ansible-doc 12345Usage: ansible-doc [-l|-F|-s] [options] [-t &lt;plugin type&gt; ] [plugin]-a # 列出所有的模块-l # 列出ansible的模块-s # 片段式显示模块的信息 8.补充 123456789[name] #分组name=CentOS-$releasever - Base - mirrors.aliyun.com #这个分组的名字failovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ #分组的url,叫baseurl http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/ http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/gpgcheck=0 # gpgcheck=1需要验证key文件,gpgcheck=0不验证keyenabled=1 #enabled=1 表示分组可用,enabled=0表示分组是不可用的gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7 #key文件 三.命令相关命令相关1.command 1234ansible web -m aommand "pwd"ansible web -a "chdir=/tmp/ mkdir /data2" # 切换到/tmp并执行pwdansible web -a "creates=/etc/ mkdir /data2" # 判断creates是否存在,真就忽略后面的操作ansible web -a "removes=/tmp/data mkdir /tmp/data2" # 判断removes是否存在,假就忽略后面的操作 2.shell 123ansible web -m shell -a "echo 'test1'|password --stdin test1" # 修改密码ansible 192.168.19.9 -m shell "/root/a.sh" # 指定远程主机上的shell脚本ansible 192.168.19.2 -m shell -a "/root/a.py" # 指定远程主机上的python文件 3.sctipt 1ansible all -m script -a "/root/a.sh" # 执行管控机的shell脚本 文件相关1.copy复制管控机文件到被管控机 123456ansible web -m copy -a "src=/etc/xxx dest=/data/xxx" # src指定源文件 dest指定目标文件ansible web -m copy -a "src=/etc/xxx dest=/data/xxx backup=yes" # backup备份ansible web -m copy -a "src=/etc/init.d dest=/data/" # 复制目录和目录下的文件到远程主机,远程主机也是一个文件夹ansible web -m copy -a "src=/etc/init.d/ dest=/data/" # 复制目录下的文件ansible web -m copy -a "src=/etc/xxx dest=/data/xxx backup=yes mode=600" # mode 指定权限,owner指定文件的属主,group用来指定属组ansible web -m copy -a "content='内容xxxxx' dest=/data10/xx.txt" # content 直接写内容 2.file 1234ansible db -m file -a "path=/data10 state=directory" # path指定地址,state=directory表示创建文件夹ansible db -m file -a "path=/data10/xxx state=touch" # state=touch 表示创建新文件ansible db -m file -a "path=/data10/test1 state=absent" # state=absent 代表删除ansible db -m file -a 'path=/data10/test10 src=/data10/test1 state=link' #src表示源文件,path是不是目标,state=link是不是创建一个软连接 3.fetch 1ansible db -m fetch -a "src=/etc/xxx dest/tmp" # src源地址(在被控机器上),dest目标地址(管控机上的地址)每个管控机的文件都生成了一个目录,会保持文件的原来目录结构 软件相关1.yum 1234ansible web -m yum -a "name=nginx state=installed" # 安装nginxansible web -m shell -a "rpm -qa | grep nginx" # 查看nginx是不是安装成功ansible web -m yum -a "name=nginx state=absent" # 卸载nginxansible web -m yum -a "name=redis,memcached" 2.pip 1ansible web -m pip -a "name=Django==1.11.15" 定时任务 cron 123ansible web -m cron -a "name=testjob minute=4 job='echo 哈哈 &gt; /tmp/xx.txt'" # 创建 name:指定的cron名字 minute:指定分钟 hour:指定小时 day:指定天 month:指定月 weekday:指定周 job:指定要执行的命令ansible web -m cron -a "name=testjob state=absent" # 删除任务ansible web -m cron -a "name=testjob minute=4 disabled=yes job='echo 哈哈 &gt; /tmp/xx.txt'" # disabled=yes表示禁用 用户相关 user 12ansible web -m user -a "name=客户1 home=/data/客户1" # 创建用户并指定家目录ansible web -m user -a "name=客户2 groups='xxx1,xxx2' home=/data/客户2" # groups='xxx1,xxx2' 指定用户的附加组 收集系统信息 setup 收集系统信息 1234567891011121314151617"ansible_all_ipv4_addresses" #ipv4简单信息"ansible_all_ipv6_addresses" #ipv6的简单信息"ansible_architecture": "x86_64", #系统架构"ansible_date_time": #系统时间"ansible_default_ipv4": #详细信息"ansible_devices": #磁盘信息"ansible_distribution_major_version": "7",#系统版本"ansible_distribution": "CentOS", #系统的发行商"ansible_distribution_file_variety": "RedHat", #系统系列"ansible_fqdn": "localhost.localdomain", #系统的主机名"ansible_hostname": "localhost",#简写主机名"ansible_kernel": "3.10.0-693.el7.x86_64", #系统的内核版本"ansible_os_family": "RedHat",# 系统的家族"ansible_processor_vcpus": 2, #cpu的个数"ansible_python_version": "2.7.5", # ansible所用python的版本 ansible web -m setup -a 'filter="*cpu*"' #filter搜索 启动应用 service 123enabled:#开机启动name:#服务的名称state: #操作 四.playbook的基本使用 playbook命令,建议:一个文件做一件事 1.基本格式 12345678ansible-playbook [options] playbook.yml-C # 干跑,检查-f FORKS # 用来做并发,来指定并发数--list-hosts #列出执行命令的主机--syntax-check # 检查语法--list-tasks #列出playbook要执行的任务列表-t TAGS, #指定要运行到tags-e EXTRA_VARS #给playbook传递变量 2.单个playbook 12345678#单个playbook- hosts: web #指定要运行命令的主机 remote_user: root # 指定运行命令的用户 tasks: #任务列表 - name: mkdir # 任务1,name是必须的 file: path=/data state=directory # 指定的模块: 模块的参数 - name: copyfile copy: src=/etc/fstab dest=/data/f 3.多个playbook 1234567891011121314##多个playbook- hosts: web remote_user: root tasks: - name: mkdir file: path=/data state=directory - name: copyfile copy: src=/etc/fstab dest=/data/f- hosts: db remote_user: root tasks: - name: wget shell: "wget -O /data/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo" 4.指定tags 123456789##指定tags- hosts: web remote_user: root tasks: - name: mkdir file: path=/data state=directory - name: copyfile copy: src=/etc/fstab dest=/data/f tags: copyfile 变量1.第一种 123456## 传递变量 -e"key=value"- hosts: web remote_user: root tasks: - name: yum &#123;&#123;pkg_name&#125;&#125; pkg yum: name=&#123;&#123;pkg_name&#125;&#125; 2.第二种 1234567- hosts: web remote_user: root vars: - pkg_name: memcached tasks: - name: yum &#123;&#123;pkg_name&#125;&#125; pkg yum: name=&#123;&#123;pkg_name&#125;&#125; 3.第三种 1234#在hosts文件里面写,值可以不同[web]192.168.19.9 pkg_name=nginx192.168.19.26 pkg_name=redis 4.第四种 12[web:vars]pkg_name=nginx 5.变量的应用顺序 1-e &gt; yml文件 &gt; hosts文件 #命令行里面是最高的,hosts文件是最低的 条件 when 条件判断 123456789- hosts: cache remote_user: root tasks: - name: copyfile1 copy: content='wusir zhenchou' dest=/tmp/a.txt when: ansible_os_family=="RedHat" #只有为真的时候才会执行上面的操作 - name: copyfile2 copy: content='alex gengchou' dest=/tmp/b.txt when: ansible_os_family=="OpenBSD" 循环with_items1.循环单个 12345678910111213- hosts: cache remote_user: root tasks: - name: create user user: name=&#123;&#123;item&#125;&#125; ## 循环下面的with_items with_items: - yuchao - yantao - name: create group group: name=&#123;&#123;item&#125;&#125;## 循环下面的with_items with_items: - yuchao2 - yantao2 2.循环嵌套 12345678910111213- hosts: cache remote_user: root tasks: - name: create group group: name=&#123;&#123;item&#125;&#125; with_items: - yuchao4 - yantao4 - name: create user user: name=&#123;&#123;item.name&#125;&#125; group=&#123;&#123;item.group&#125;&#125; #可以通过字典取值 with_items: - &#123;"name":yuchao3,"group":yuchao4&#125; - &#123;"name":yantao3,"group":yuchao4&#125; 模板文件1.模板的基本使用 12345678910- hosts: cache remote_user: root tasks: - name: install redis yum: name=redis - name: copyfile template: src=redis.conf.j2 dest=/etc/redis.conf ## 模板基于jinja2 - name: start service: name=redis state=started #模板文件放在templates,可以直接用相对路径去调用配置文件 roles(高级使用) 作用 结构清晰 可以重用 结构 12345tasks #目录是必须的,存放任务templates #是存放模板vars #用来存放变量 ### 切记,不能加-,加-报错files #用来存放文件mkdir -p &#123;nginx,uwsgi,mysql&#125;/&#123;tasks,templates,vars,files&#125; #创建目录结构的命令 补充生成公私钥 1ssh-keygen 复制公钥到远程主机 1ssh-copy-id 192.168.19.99 ping命令发送的是ICMP协议 查看用户相关 1234tail -l /etc/shadow 查看最后一个用户echo "testl" | password --stdin testl 设置用户密码,不需要二次确useradd # 创建用户默认的家目录在/home -d 可以指定用户的家目录groupadd # 用来创建用户组,用户组没有家目录 创建链接 12ln 创建硬链接 链接文件变更 源文件不变ln -s 创建软连接 链接文件变更 源文件变 pip的基本使用 123pip freeze &gt; file # 给当前的python模块做快照pip install -r xxx.txt # 安装pip list # 查看所有的python模块 crontab定时任务 1234567* */5 * * * job #/n 表示每隔n 0 */5 * * 3,6 job #3,6 表示周三和周六## 切记 最前面不能用*,表示每时每刻都在执行,一定要有一个时间## 应用场景: 打包日志,定期的同步时间,备份-e # 编辑-l # 列出-r # 删除 启动应用 12systemctl restart nginx # centos7中的操作应用service nginx restart # centos6里面的操作 查看系统内存使用量 1free -m 查看系统的内存使用量 Ad-hoc:命令行的意思 mv 的使用 1mv redis.conf&#123;,.j2&#125; == mv redis.conf redis.conf.j2 yum和rpm的基本使用 1234yum remove 卸载rpm redhat pk manageyum 自动解决依赖关系rpm 不会自动解决依赖关系 CI和CD的基本使用 12CI 持续交付 jenkins maven war包CD 持续集成 脚本去做 docker (最大的一个作用,到处运行 ) k8s 暂时关闭防火墙 1setenforce 0 #暂时关闭selinux uwsgi的配置 1234567891011121314151617[uwsgi]http = :8000 #端口#the local unix socket file than commnuincate to Nginxsocket = /data/mysite/mysite.socket #socket 只能本机使用# the base directory (full path)chdir = /data/mysite #当前工作目录# Django's wsgi filewsgi-file = mysite/wsgi.py #要执行的文件# maximum number of worker processesprocesses = 4 #进程数#thread numbers startched in each worker processthreads = 2 #线程数# clear environment on exitvacuum = truedaemonize = /data/mysite/uwsgi.log #后台启动,并提供日志py-autoreload=1 #py文件变更后uwsgi自动重启 kill -9 可以杀死父进程 启动uwsgi 1uwsgi --http :8000 --module mysite.wsgi ## --http 启动的端口 --module 项目.wsgi文件]]></content>
      <categories>
        <category>运维开发之路</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
</search>
